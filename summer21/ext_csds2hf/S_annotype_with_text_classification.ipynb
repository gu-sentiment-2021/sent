{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce192a0a-9703-4393-ab55-c24cdd1222cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece]\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f96831-ae2c-4adc-8730-cc07b9b5ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ab8c6-603d-40af-875b-5b6f68f9accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "\n",
    "device_string = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_hf = 0 if torch.cuda.is_available() else -1\n",
    "device = torch.device(device_string)\n",
    "print(\"Device:\", device)\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b4ffa-77dd-4413-bd5b-a33752467915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup wandb\n",
    "\n",
    "wandb.login()\n",
    "%env WANDB_PROJECT=annotype_text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a085e-a728-4a8e-8f75-193820216f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "IGNORED_CLASSES = [] # e.g. set it to ['sentiment'] to remove the objects with the type of sentiment, from the databset\n",
    "MODEL_NAME = 'distilbert-base-cased'\n",
    "INPUT_TYPE = 'TEXT_HEAD' # Possible values: 'TEXT_HEAD', 'TEXT_ONLY', 'HEAD_ONLY'\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LOGGING_STEPS = 100\n",
    "EVAL_STRATEGY = 'steps'\n",
    "SAVE_STRATEGY = 'steps'\n",
    "WEIGHT_DECAY = 0.1\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "NUM_TRAIN_EPOCHS = 10\n",
    "CALLBACKS = [EarlyStoppingCallback(4)]\n",
    "SEED = 0\n",
    "DATA = 'MPQA3.0_v211021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data & augmented data urls\n",
    "\n",
    "data_name_to_google_drive_url = {\n",
    "    'MPQA3.0_v211021': 'https://drive.google.com/file/d/1e-pDfZ2cyBzgD9MEerP9YCcDnPvIQuGo/view?usp=sharing',\n",
    "}\n",
    "\n",
    "# Get direct download link\n",
    "def get_download_url_from_google_drive_url(google_drive_url):\n",
    "    return f'https://drive.google.com/uc?id={google_drive_url.split(\"/\")[5]}&export=download'\n",
    "\n",
    "# Data URL\n",
    "google_drive_url = data_name_to_google_drive_url[DATA]\n",
    "data_url = get_download_url_from_google_drive_url(google_drive_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a21171-f03f-45d1-9293-4bcb4183eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65615ad0-5c82-4079-be75-122e75d8441c",
   "metadata": {
    "id": "faEGTkqS7Hp4"
   },
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28384faa-36ff-4caa-b598-4946d990c7ba",
   "metadata": {
    "id": "nyymAwVCYxpf"
   },
   "outputs": [],
   "source": [
    "# Fetch the dataset\n",
    "\n",
    "FETCH_FROM_WEB = True ### Set it to true, to download the datasets from github and google drive ###\n",
    "\n",
    "if FETCH_FROM_WEB:\n",
    "    response = urlopen(data_url)\n",
    "    csds_collection = json.loads(response.read())\n",
    "else:\n",
    "    file_address = '..\\\\json2csds\\\\data.json'\n",
    "    with open(file_address) as file:\n",
    "        csds_collection = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee725191-73e1-47ee-a8d6-1465aa32b6f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyWk_HPcZDG_",
    "outputId": "c7799125-54d5-431d-80bf-4bc4f1abdc78"
   },
   "outputs": [],
   "source": [
    "# Preparing inputs and targets\n",
    "\n",
    "inputs_text = []\n",
    "inputs_head = []\n",
    "inputs_tuple_text_head = []\n",
    "targets_annotype = []\n",
    "n_samples = 0\n",
    "\n",
    "for csds_object in csds_collection['csds_objects']:\n",
    "    if csds_object['annotation_type'] not in IGNORED_CLASSES:\n",
    "        inputs_text += [csds_object['text']]\n",
    "        inputs_head += [csds_object['head']]\n",
    "        inputs_tuple_text_head += [(csds_object['text'], csds_object['head'])]\n",
    "        targets_annotype += [csds_object['annotation_type']]\n",
    "        n_samples += 1\n",
    "\n",
    "i = 128 # A sample\n",
    "print(f'inputs and targets for {i+1}-th csds object (out of {n_samples}):')\n",
    "print('inputs_text:\\t\\t', inputs_text[i])\n",
    "print('inputs_head:\\t\\t', inputs_head[i])\n",
    "print('inputs_tuple_text_head:\\t', inputs_tuple_text_head[i])\n",
    "print('targets_annotype:\\t', targets_annotype[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3545fa6-6969-45a9-b020-882f78a1322b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkFSm35wDmmJ",
    "outputId": "b401d0a1-57e7-4f7a-c3ff-d64c2ae5b602"
   },
   "outputs": [],
   "source": [
    "# Count the number of each annotation type and extract the labels\n",
    "\n",
    "num_annotype = {}\n",
    "for annotype in targets_annotype:\n",
    "    num_annotype[annotype] = num_annotype.get(annotype, 0) + 1\n",
    "print(sorted(num_annotype.items()))\n",
    "classes = sorted(list(num_annotype.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e078f-7b8d-4099-a123-7d2c521bcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map for class ids and class names\n",
    "\n",
    "classname2classid = {classes[i]:i for i in range(len(classes))}\n",
    "classid2classname = {i:classes[i] for i in range(len(classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4bb5a3-e693-4525-a939-40937b6848ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classname2classid mapping\n",
    "\n",
    "y = [classname2classid[i] for i in targets_annotype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e31d0-9937-447b-a8f3-459c9310ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split the dataset into training and validation sets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    np.array(inputs_tuple_text_head), y, test_size=0.2,\n",
    "    random_state=SEED, shuffle=True, stratify=y\n",
    ")\n",
    "\n",
    "X_train_text, X_train_head = X_train[:, 0].tolist(), X_train[:, 1].tolist()\n",
    "X_val_text,   X_val_head   = X_val[:, 0].tolist(),   X_val[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab1607-bad3-4222-b15c-0ac3ec39acd5",
   "metadata": {},
   "source": [
    "# Preparing the model and torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732e3ad-d4f0-4ee5-8d6c-0bdc523539f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(classes), resume_download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ceb2ae-6206-4e83-92b7-46b7a22c93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the inputs\n",
    "\n",
    "if INPUT_TYPE == 'TEXT_HEAD':\n",
    "    X_train_tokenized = tokenizer(X_train_text, X_train_head, truncation=True)\n",
    "    X_val_tokenized   = tokenizer(X_val_text,   X_val_head,   truncation=True)\n",
    "\n",
    "if INPUT_TYPE == 'TEXT_ONLY':\n",
    "    X_train_tokenized = tokenizer(X_train_text, truncation=True)\n",
    "    X_val_tokenized   = tokenizer(X_val_text,   truncation=True)\n",
    "\n",
    "if INPUT_TYPE == 'HEAD_ONLY':\n",
    "    X_train_tokenized = tokenizer(X_train_head, truncation=True)\n",
    "    X_val_tokenized   = tokenizer(X_val_head,   truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af2aa8-1911-4a04-bd60-6b0d87c0896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest input size\n",
    "\n",
    "t = 0\n",
    "for i in X_train_tokenized['input_ids']:\n",
    "    t = max(t, len(i))\n",
    "for i in X_val_tokenized['input_ids']:\n",
    "    t = max(t, len(i))\n",
    "print(\"Maximum input length:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb9aae-3aa3-47a4-befe-cf9a80978ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b844adf-e40c-4712-b714-79ece6327c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846d85b-c0c8-4d6a-a273-efab206be652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    targets = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    labels = [i for i in range(len(classes))] # [0, 1, 2, ..., len(classes)-1]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        targets, preds, labels=labels, zero_division=0, average='macro'\n",
    "    )\n",
    "    precision_list, recall_list, f1_list, _ = precision_recall_fscore_support(\n",
    "        targets, preds, labels=labels, zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    decimals = 4\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': np.around(f1, decimals),\n",
    "        'precision': np.around(precision, decimals),\n",
    "        'recall': np.round(recall, decimals),\n",
    "        'f1-list': np.around(f1_list, decimals).tolist(),\n",
    "        'precision-list': np.around(precision_list, decimals).tolist(),\n",
    "        'recall-list': np.round(recall_list, decimals).tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49eb20-a378-438d-abb3-707553bc5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'models/pretrain_'+MODEL_NAME+'_'+INPUT_TYPE,\n",
    "    overwrite_output_dir = True,\n",
    "    per_device_train_batch_size = TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size = EVAL_BATCH_SIZE,\n",
    "    evaluation_strategy = EVAL_STRATEGY,\n",
    "    logging_steps = LOGGING_STEPS,\n",
    "    save_strategy = SAVE_STRATEGY,\n",
    "    save_steps = LOGGING_STEPS,\n",
    "    save_total_limit = 2,\n",
    "    weight_decay = WEIGHT_DECAY,\n",
    "    num_train_epochs = NUM_TRAIN_EPOCHS,\n",
    "    load_best_model_at_end = LOAD_BEST_MODEL_AT_END,\n",
    "    dataloader_num_workers = NUM_WORKERS,\n",
    "    seed = SEED,\n",
    "    report_to = 'wandb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfde22e-6ec5-409d-9d5f-bafb21d8d0e9",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ddbda8-de03-4529-8dd0-b9501f85b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free some space\n",
    "\n",
    "if 'trainer' in globals():\n",
    "    del trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705257c-d00c-4257-9f91-f1bfd918776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = CALLBACKS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56cd33-ce58-478f-a576-78fdb54f6322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa7c92-b9bb-48c4-b963-0ffa1658ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703f20c-2215-406d-95c9-e98ed0fd467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "\n",
    "targets = pred.label_ids\n",
    "preds = pred.predictions.argmax(-1)\n",
    "\n",
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap='Blues')\n",
    "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "    plt.ylabel('True annotation type')\n",
    "    plt.xlabel('Predicted annotation type');\n",
    "\n",
    "cm = confusion_matrix(targets, preds)\n",
    "df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3fcac-8214-4033-8fce-1b11f475c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
