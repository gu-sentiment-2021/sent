{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "augment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1peowkMAS2HV"
      },
      "source": [
        "Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZK5fRC8CQaF",
        "outputId": "31713d05-d6f9-4658-fc85-8a15c59cb598"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install nlpaug\n",
        "!pip install wget\n",
        "!pip install matplotlib\n",
        "!pip install requests"
      ],
      "execution_count": 555,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.7/dist-packages (1.1.8)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (2.23.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug) (3.0.4)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkPuz74RS9ui"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TkzLraOCL89"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import nlpaug\n",
        "import nlpaug.augmenter.word as naw\n",
        "from transformers import pipeline\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import nltk\n"
      ],
      "execution_count": 556,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyw04s2vvLME",
        "outputId": "4b7de14d-81dc-48c7-beac-deb48ea3b703"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 557,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 557
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myJ9G6jvTC3t"
      },
      "source": [
        "Mounting drive and loading the JSON data file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oCoarwlw5Bh",
        "outputId": "0410e35a-679b-430b-959c-63e8e201059f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 558,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKhbjrNKyBcS"
      },
      "source": [
        "path = \"/content/drive/My Drive/csds_storage/\"\n",
        "# Loading the saved JSON file.\n",
        "with open(path + 'data.json') as json_file:\n",
        "    data = json.load(json_file)\n"
      ],
      "execution_count": 559,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yX_FSKkTvK8"
      },
      "source": [
        "A Python method for processing JSON data in order to put the head and text of CSDS objects (which containing intensity attribute) in a dictionary for next uses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLP6xksbshnz"
      },
      "source": [
        "def process_csds_objects(data):\n",
        "    \"\"\"\n",
        "    It receives the JSON data which is the JSON format of the CSDS, Target and Agent\n",
        "    objects, and then extract CSDS objects (which are in JSON format) in order to\n",
        "    process each of them. \n",
        "    \"\"\"\n",
        "\n",
        "    lst = {'high-extreme': ([], []), 'extreme': ([], []), 'medium': ([], []), 'medium-high': ([], []), 'low': ([], []), 'high': ([], []), 'low-medium': ([], [])}\n",
        "\n",
        "    for item in data['csds_objects']:\n",
        "        #Check if this CSDS object has 'intensity' value, because the 'intensity' value if optional.\n",
        "        if item['intensity']: \n",
        "            lst[item['intensity']][0].append(item['text'])\n",
        "            lst[item['intensity']][1].append(item['head'])    \n",
        "    return lst\n"
      ],
      "execution_count": 560,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uQNLXu3TLO6"
      },
      "source": [
        "A Python method, which augments with BERT, uses **insertion** and does not change the **head span**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci8os4SLyM74"
      },
      "source": [
        "def bert_augmenter_simple(lst, model_name='bert-base-cased'):\n",
        "    TOPK=20 #default=100\n",
        "    ACT='insert' #\"substitute\"\n",
        " \n",
        "    aug_bert = naw.ContextualWordEmbsAug(\n",
        "        model_path=model_name,\n",
        "        #device='cuda',\n",
        "        action=ACT, top_k=TOPK)\n",
        "\n",
        "    augmented_text = aug_bert.augment(lst)\n",
        "    return augmented_text "
      ],
      "execution_count": 561,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtRR2nOuUUei"
      },
      "source": [
        "Using the defined method to receive the dictionary of intensity classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntWtjL4N8i1d"
      },
      "source": [
        "# Main code\n",
        "classes_lst = process_csds_objects(data)\n"
      ],
      "execution_count": 562,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9v4lAW2Uhuq"
      },
      "source": [
        "The following dictionary describes how many times we need to augment each class of data or how many items we want to add to the original data. Thus addition of data could be done in 2 ways: <br>\n",
        "A) Using 'mul' and the corresponding number (we call it **n**), that is all data is augmented by the number **n**.\n",
        "<br>\n",
        "B) Using 'add' and the corresponding number (we call it **n**), that is the data items are shuffled first, then the first **n** data items are collected and they will be augmented. So **n** items are added to our original set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucnXB8f399Ui"
      },
      "source": [
        "aug_rpt_cnt = {'high-extreme': ['mul', 10], 'extreme': ['mul', 50], \n",
        "               'medium': ['add', 30], 'medium-high': ['mul', 2], \n",
        "               'low': ['mul', 1], 'high': ['add', 150], 'low-medium': ['add', 200]}\n"
      ],
      "execution_count": 563,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2tUCG2CWhNm"
      },
      "source": [
        "The following list defines the classes which we want to augment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir5WzFv5TqON"
      },
      "source": [
        "to_do_lst = [] #['low', 'high', 'low-medium', 'high-extreme', 'extreme','medium', 'medium-high']"
      ],
      "execution_count": 564,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_G0S6kyYWcz"
      },
      "source": [
        "Creating a BERT augmenter just like what we did in the previously defined method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bioGaTSRYJyR"
      },
      "source": [
        "model_name='bert-base-cased'\n",
        "TOPK=20 #default=100\n",
        "ACT='insert' #\"substitute\"\n",
        "\n",
        "aug_bert = naw.ContextualWordEmbsAug(\n",
        "    model_path=model_name,\n",
        "    #device='cuda',\n",
        "    action=ACT, top_k=TOPK)\n"
      ],
      "execution_count": 565,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUxPlwDbWws_"
      },
      "source": [
        "The following block augments each class of data items (just the classes we want to augment, present in the ***to_do_lst*** list), Based on our previous descriptions we can add to data items or multiply the count of data items by our specified number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeQ3gaJsUBDu"
      },
      "source": [
        "for cl in to_do_lst:\n",
        "    aug_type, aug_cnt = aug_rpt_cnt[cl][0], aug_rpt_cnt[cl][1]\n",
        "    X = []\n",
        "    y = []\n",
        "    if aug_type == 'mul':\n",
        "        cnt = aug_cnt\n",
        "        lst_text = classes_lst[cl][0]\n",
        "        lst_head = classes_lst[cl][1]\n",
        "        while cnt > 0:\n",
        "            augmented_text = aug_bert.augment(lst_text)\n",
        "            for ii in range(len(augmented_text)):\n",
        "                X.append((augmented_text[ii], lst_head[ii]))\n",
        "            y += [cl] * len(lst_head)\n",
        "            cnt -= 1\n",
        "    else:\n",
        "        cnt = aug_cnt\n",
        "        lst_text = classes_lst[cl][0]\n",
        "        lst_head = classes_lst[cl][1]\n",
        "        c = list(zip(lst_text, lst_head))\n",
        "        random.shuffle(c)\n",
        "        lst_text, lst_head = zip(*c)\n",
        "        lst_text, lst_head = list(lst_text), list(lst_head)\n",
        "        augmented_text = aug_bert.augment(lst_text[: cnt])\n",
        "        for ii in range(len(augmented_text)):\n",
        "            X.append((augmented_text[ii], lst_head[ii]))\n",
        "        y += [cl] * cnt\n",
        "\n",
        "    #\n",
        "    data_aug = {\n",
        "    'aug_class': cl,\n",
        "    'X': X,\n",
        "    'y': y\n",
        "    }\n",
        "    del X, y\n",
        "    # The 'path' defines where the augmented data will be written to.\n",
        "    path = \"/content/drive/My Drive/csds_storage/\"\n",
        "    # Saving JSON file.\n",
        "    with open(path + cl + '_data_aug.json',  'w') as outfile:\n",
        "        json.dump(data_aug, outfile, indent=4)\n",
        "\n",
        "    print(f'Class {cl} augmentation completed!')"
      ],
      "execution_count": 566,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi-8viE73SD8",
        "outputId": "c6560bd1-4436-408c-eb0d-6d84c2e58bf5"
      },
      "source": [
        "# Create a reference variable for Class WhitespaceTokenizer\n",
        "unmasker = pipeline('fill-mask', model=model_name)    \n",
        "     \n"
      ],
      "execution_count": 567,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh89CIACusNr"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 568,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nkajVIIAxiO"
      },
      "source": [
        "def is_valid_token(token):\n",
        "    if len(token) < 2 or token in stopwords or token.find('-') > -1 or token.find('.') > -1:\n",
        "        return False\n",
        "    return True"
      ],
      "execution_count": 569,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZLBXm4thSYa"
      },
      "source": [
        "def clean_string(st):\n",
        "    return st.replace('  ', ' ').replace(' ,', ',').replace(' .', '.')"
      ],
      "execution_count": 570,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4kL1AwMJRnU"
      },
      "source": [
        "total_costum_text_augments = []\n",
        "total_costum_head_augments = []"
      ],
      "execution_count": 571,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iclbgTvH5pqr"
      },
      "source": [
        "EXTEND_CNT = 2\n",
        "PROB = 0.5\n",
        "SEL_PROB = 0.5\n"
      ],
      "execution_count": 572,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVSPzDp1C3kV"
      },
      "source": [
        "def custom_augment_rec_simple(text_tokens, head_tokens, i, j, head_beg, head_end, n):\n",
        "    if i == n:\n",
        "        total_costum_text_augments.append(clean_string(str(' '.join(text_tokens))))\n",
        "        total_costum_head_augments.append(clean_string(str(' '.join(head_tokens))))\n",
        "    else:\n",
        "        if i >= head_beg and i < head_end:\n",
        "            custome_augment_rec(text_tokens, head_tokens, i+1, j+1, head_beg, head_end, n)\n",
        "        else:\n",
        "            custom_augment_rec(text_tokens, head_tokens, i+1, j, head_beg, head_end, n)\n",
        "        if is_valid_token(text_tokens[i]):\n",
        "            text_tokens[i] = '[MASK]'        \n",
        "            result = unmasker(str(' '.join(text_tokens)))\n",
        "\n",
        "            if i >= head_beg and i < head_end:\n",
        "                for k in range(EXTEND_CNT):\n",
        "                    text_tokens[i] = result[k]['token_str']\n",
        "                    head_tokens[j] = text_tokens[i]\n",
        "                    custom_augment_rec(text_tokens, head_tokens, i+1, j+1, head_beg, head_end, n)\n",
        "            else:\n",
        "                for k in range(EXTEND_CNT):\n",
        "                    text_tokens[i] = result[k]['token_str']\n",
        "                    custom_augment_rec(text_tokens, head_tokens, i+1, j, head_beg, head_end, n)\n"
      ],
      "execution_count": 573,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh-jDUsxIs46"
      },
      "source": [
        "def custom_augment_rec(text_tokens, head_tokens, i, j, head_beg, head_end, n):\n",
        "    if i == n:\n",
        "        total_costum_text_augments.append(clean_string(str(' '.join(text_tokens))))\n",
        "        total_costum_head_augments.append(clean_string(str(' '.join(head_tokens))))\n",
        "    else:\n",
        "        if i >= head_beg and i < head_end:\n",
        "            custom_augment_rec(text_tokens, head_tokens, i+1, j+1, head_beg, head_end, n)\n",
        "        else:\n",
        "            custom_augment_rec(text_tokens, head_tokens, i+1, j, head_beg, head_end, n)\n",
        "\n",
        "      \n",
        "        if np.random.rand() < PROB and is_valid_token(text_tokens[i]):\n",
        "            text_tokens[i] = '[MASK]'        \n",
        "            result = unmasker(str(' '.join(text_tokens)))\n",
        "\n",
        "            if i >= head_beg and i < head_end:\n",
        "                for k in range(EXTEND_CNT):\n",
        "                    text_tokens[i] = result[k]['token_str']\n",
        "                    head_tokens[j] = text_tokens[i]\n",
        "                    custom_augment_rec(text_tokens, head_tokens, i+1, j+1, head_beg, head_end, n)\n",
        "            else:\n",
        "                for k in range(EXTEND_CNT):\n",
        "                    text_tokens[i] = result[k]['token_str']\n",
        "                    custom_augment_rec(text_tokens, head_tokens, i+1, j, head_beg, head_end, n)\n"
      ],
      "execution_count": 574,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJg0rHgLcnZS"
      },
      "source": [
        "def custom_augment_ite(text_tokens, head_tokens, head_beg, head_end):\n",
        "    j = 0\n",
        "    for i in range(len(text_tokens)):\n",
        "        if np.random.rand() < PROB and is_valid_token(text_tokens[i]):\n",
        "            text_tokens[i] = '[MASK]'        \n",
        "            result = unmasker(str(' '.join(text_tokens)))\n",
        "            t = 0 if np.random.rand() < SEL_PROB else 1\n",
        "            if i >= head_beg and i < head_end:\n",
        "                text_tokens[i] = result[t]['token_str']\n",
        "                head_tokens[j] = text_tokens[i]\n",
        "                j += 1\n",
        "            else:\n",
        "                text_tokens[i] = result[t]['token_str']\n",
        "\n",
        "\n",
        "    total_costum_text_augments.append(clean_string(str(' '.join(text_tokens))))\n",
        "    total_costum_head_augments.append(clean_string(str(' '.join(head_tokens))))"
      ],
      "execution_count": 575,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecsBBFo424XA"
      },
      "source": [
        "def custom_augment_one(text, head):\n",
        "    # Clear the answers list\n",
        "    total_costum_text_augments.clear()\n",
        "    total_costum_head_augments.clear()\n",
        "    text_first = text[: text.find(head)]\n",
        "    text_first_tokens = word_tokenize(text_first)\n",
        "    text_tokens = word_tokenize(text)\n",
        "    head_tokens = word_tokenize(head)\n",
        "    custom_augment_rec(text_tokens, head_tokens, 0, 0, len(text_first_tokens), len(text_first_tokens)+len(head_tokens), len(text_tokens))\n",
        "    return total_costum_text_augments, total_costum_head_augments\n"
      ],
      "execution_count": 576,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUEeQlXDdwqA"
      },
      "source": [
        "def custom_augment(text, head, num=3):\n",
        "    # Clear the answers list\n",
        "    total_costum_text_augments.clear()\n",
        "    total_costum_head_augments.clear()\n",
        "    text_first = text[: text.find(head)]\n",
        "    text_first_tokens = word_tokenize(text_first)\n",
        "    text_tokens = word_tokenize(text)\n",
        "    head_tokens = word_tokenize(head)\n",
        "    for k in range(num):\n",
        "        custom_augment_ite(text_tokens, head_tokens, len(text_first_tokens), len(text_first_tokens)+len(head_tokens))\n",
        "    return total_costum_text_augments, total_costum_head_augments\n"
      ],
      "execution_count": 577,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtMsMcUJDvW_",
        "outputId": "eebe5d99-0024-45bd-c8b7-44ccb4a0ce09"
      },
      "source": [
        "text = \"Indeed, the U.S Administration, which claims to advocate international human rights issues and which tries to play the role of a world policeman, is actually condoning the most atrocious human rights violations committed in the United States itself.\"\n",
        "head = \"most atrocious human rights violations\"\n",
        "\n",
        "#text = 'Trump lost the 2020 presidential election to Biden but refused to concede.'\n",
        "#head = 'refused'\n",
        "augmented_text, augmented_head = custom_augment(text, head, num=10)\n",
        "#_ = [print(n) for n in augmented_text]\n",
        "for i in range(len(augmented_text)):\n",
        "    print(f'{augmented_text[i]} ===>> {augmented_head[i]}')\n"
      ],
      "execution_count": 578,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moreover, the U.S Administration, which claims to advocate international human rights law and which tries to play the role of a world court, is also committing the most atrocious human rights violations committed in the Gulf War itself. ===>> violations atrocious human rights violations\n",
            "Meanwhile, the U.S Administration, which seeks to reform its human rights law and which tries to play the role of a supreme court, is also facing the most serious civil rights violations, in the Iraq War itself. ===>> serious civil human rights violations\n",
            "Meanwhile, the U.S Administration, which wants to reform its human rights law and which tries to play the part of a military dictator, is also facing the most serious civil rights violations, in the Vietnam war itself. ===>> civil civil human rights violations\n",
            "Meanwhile, the U.S., which wants to reform its human rights law and which wants to play the part of a democratic dictator, is also committing the most severe civil liberties violation, in the nuclear war itself. ===>> severe liberties violation rights violations\n",
            "However, the U.S., which wants to change its human right law and which wants to play the part of a democratic dictator, is now committing the most serious civil rights violations, in the nuclear war itself. ===>> serious rights violations rights violations\n",
            "Meanwhile, the U.S., which wants to change its human right position and which wants to play the part of a military dictator, is now facing the most serious civil rights violations, in the nuclear war itself. ===>> serious rights violations rights violations\n",
            "Meanwhile, the U.S., which needs to change its human right position and which wants to play the part of a military dictatorship, is now facing the most serious civil rights violations, in the civil society itself. ===>> serious rights violations rights violations\n",
            "Meanwhile, the U.S., which needs to maintain its political right, and which needs to play the part of a military dictatorship, is now facing the most serious human rights violation, in the civil society itself. ===>> human violation violations rights violations\n",
            "However, the U.S., which needs to protect its political interests, and which needs to play the role of a military dictatorship, is now facing the most serious human rights violations, in the civil society itself. ===>> violations violation violations rights violations\n",
            "Meanwhile, the U.S., which needs to pursue its political interests, and which needs to play the role of a military dictatorship, is now facing the most serious human right violations, in the civil war itself. ===>> human right violations rights violations\n"
          ]
        }
      ]
    }
  ]
}